import re
import time
import numpy as np
import sounddevice as sd
import os
from typing import Union
from datasets import load_dataset, Dataset, interleave_datasets, Audio, DatasetDict
import json

import torch
import transformers

SEED = 42

class New_PharmaIntent_Dataset:

    # INTENT_LABEL = ["other_intents", "retrieve_med", "search_med", "enquire_suitable_med"]
    INTENT_LABEL = [
        "enquire_info",
        "retrieve",
        "enquire_location",
        "enquire_suitable_med",
        "general_chat",
        "set_furniture",
        "set_software"
    ]
    # O: irelevant B: beginning I: inside
    NER_LABEL = ["O", "B-ACE_Inhibitor", "I-ACE_Inhibitor", "B-Metformin", "I-Metformin", "B-Atorvastatin", "I-Atorvastatin", "B-Amitriptyline", "I-Amitriptyline",]

    datasets: dict[DatasetDict]
    train_ds: Dataset
    test_ds: Dataset
    valid_ds: Dataset

    def __init__(self, repo_id: str, config: dict):
        self.repo_id = repo_id

        self._set_metadata()
        self._call_dataset_workflow(config) if config["use_exist"] else None

    def _call_dataset_workflow(self, config:dict):

        # Call each language dataset, then merge them and split into train and test
        self.datasets = {}
        
        for lang in config["languages"]:
            lang_ds = load_dataset(self.repo_id, name=lang)
            processed_lang_ds = lang_ds.map(New_PharmaIntent_Dataset.postdownload_process)
            self.datasets[lang] = processed_lang_ds

        # Merge the datasets and set split with the merged one
        if (config["merge_language"]):
            self.group_lang()

        print("Dataset loaded successfully! \n")
    

    def _set_metadata(self):
        self.audio_col = "Audio_Path"
        self.speech_col = "Speech"
        self.intent = "Intent"
        self.ner_tag = "NER_Tag"

    def set_splits_by_lang(self, lang):
        self.train_ds = self.datasets[lang]["train"]
        self.valid_ds = self.datasets[lang]["valid"]
        self.test_ds = self.datasets[lang]["test"]

        print(f"{lang} dataset loaded!")
        print("Train:", len(self.train_ds), "Valid:", len(self.valid_ds), "Test:", len(self.test_ds))
        return

    def group_lang(self):
        self.train_ds = interleave_datasets([ds["train"] for ds in self.datasets.values()], stopping_strategy="all_exhausted")
        self.test_ds = interleave_datasets([ds["test"] for ds in self.datasets.values()], stopping_strategy="all_exhausted")
        self.valid_ds = interleave_datasets([ds["valid"] for ds in self.datasets.values()], stopping_strategy="all_exhausted")

        print(f"Merged dataset loaded!")
        print("Train:", len(self.train_ds), "Valid:", len(self.valid_ds), "Test:", len(self.test_ds))
        return
    
    def create_vocab_file(self, output_dir: str, vocab_filename: str = "vocab.json"):
        """
        Creates a vocabulary file from the sentences in the dataset.

        Args:
            output_dir (str): Directory to save the vocabulary file.
            vocab_filename (str): Name of the vocabulary file (default: "vocab.json").
        """

        vocab = {}
        

        # Combine all sentences from train, validation, and test datasets
        print("Extracting sentences from the dataset...")
        all_sentences = (
            self.train_ds["Speech"] +
            self.valid_ds["Speech"] +
            self.test_ds["Speech"]
        )


        # add special tokens
        vocab["[UNK]"] = len(vocab)
        vocab["[PAD]"] = len(vocab)
        vocab["|"] = vocab[" "]
        del vocab[" "]

        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)

        # Save vocabulary to file
        vocab_path = os.path.join(output_dir, vocab_filename)
        print(f"Saving vocabulary to {vocab_path}...")
        with open(vocab_path, "w", encoding="utf-8") as vocab_file:
            json.dump(vocab, vocab_file, ensure_ascii=False, indent=4)

        print("Vocabulary file created successfully!")

    # Use for validation / deployment, return 4-item list (padded)
    @staticmethod
    def check_NER(NER_Tag: Union[str, list[str]], fill_na=True) -> list[str]:
        
        if type(NER_Tag) == str:
            tag_list = list(NER_Tag)
        else:
            tag_list = NER_Tag
        
        detect_med = set()

        for token in tag_list:
            if token != "0" and New_PharmaIntent_Dataset.NER_LABEL[int(token)][2:] != "":
                detect_med.add(New_PharmaIntent_Dataset.NER_LABEL[int(token)][2:]) # [2:] -> remove the beginning and interim tag

        listed_med = list(detect_med)

        if fill_na:
            listed_med = listed_med + ["Empty"] * (4 - len(listed_med))

        return listed_med

    @staticmethod
    def post_process_med(output: list[dict]) -> list[4]:
        """
        Processing output list generated by NER Pipeline. Return a list of medicine lists
        """
        ner_tag = []
        for tkn in output:
            ner_tag.append(tkn["entity"][-1])
        detect_med = New_PharmaIntent_Dataset.check_NER(ner_tag)
        return detect_med
    # # Truncate/pad audio to 5 seconds (5 * 16000 samples for 16kHz sampling rate) for fair model comparison
    # @staticmethod
    # def _preprocess_data(example):

    #     # Process audio
    #     max_length = 5 * 16000  # 5 seconds at 16kHz
    #     audio_array = example["Audio"]["array"]
        
    #     # Truncate if longer than 5 seconds
    #     if len(audio_array) > max_length:
    #         audio_array = audio_array[:max_length]
    #     # Pad with zeros if shorter than 5 seconds
    #     elif len(audio_array) < max_length:
    #         padding = np.zeros(max_length - len(audio_array), dtype=audio_array.dtype)
    #         audio_array = np.concatenate([audio_array, padding])
        
    #     example["Audio"]["array"] = audio_array

    #     return example

    @staticmethod
    def postdownload_process(example):

        tokenized_speech = []
        ner_labels = []
        speech = example["Speech"]

        # Preprocess NER_Tag
        if example["NER_Tag"][0] == "'":
            ner_tag = example["NER_Tag"][1:]
        else:
            ner_tag = example["NER_Tag"]

        # Preprocess Speech
        tokens = hybrid_split(speech)
        tokenized_speech = tokens

        # Preprocess Intent
        num_intent = New_PharmaIntent_Dataset.INTENT_LABEL.index(example["Intent"])
        
        # Ensure NER_Tag length matches the number of tokens
        if len(ner_tag) != len(tokens):
            raise ValueError(f"Mismatch between tokens and NER_Tag: {speech}")

        ner_labels = [int(tag) for tag in ner_tag]
        
        example["Tokenized_Speech"] = tokenized_speech
        example["NER_Labels"] = ner_labels
        example["Intent_Label"] = num_intent

        return example

    @staticmethod
    def  build_new_dataset(repo_id, config):
        ds = load_dataset("csv", data_files=config["data_file"], split="train")

        # Load Audio into dataset
        ds = ds.cast_column("Audio_Path", Audio(sampling_rate=16000))
        ds = ds.rename_column("Audio_Path", "Audio")

        # ds = ds.map(PharmaIntent_Dataset.preprocess_audio)

        total_amt = len(ds)

        train_amt = int(total_amt * config["train_ratio"] - (total_amt * config["train_ratio"] % 16))
        test_amt = int(total_amt - train_amt)

        splited_ds = ds.train_test_split(test_size=test_amt, shuffle=True, seed=SEED)
        feed_ds = splited_ds["train"].train_test_split(test_size=1-config["train_ratio"], shuffle=True, seed=SEED)

        train_ds = feed_ds["train"]
        validate_ds = feed_ds["test"]
        test_ds = splited_ds["test"]

        doneDS = DatasetDict({
            "train": train_ds,
            "valid": validate_ds,
            "test": test_ds
        })

        # English: 118 train, 27 valid, 34 test -> 179
        # Cantonese: 114 train, 30 valid, 40 test -> 184
        # Eng_Can: 40 train, 6 valid, 14 test -> 60
        # Can_Eng: 35 train, 14 valid, 11 test -> 60

        # Can_Eng: Cantonese as main, English as secondary
        # Eng_Can: English as main, Cantonese as secondary

        for l in ["English", "Cantonese"]:

            pushing_ds = doneDS.filter(lambda example: example["Language"] == l)
            pushing_ds = pushing_ds.remove_columns("Language")

            print("Pushing config: " + l + "\n")
            print(pushing_ds)

            pushing_ds.push_to_hub(
                repo_id=repo_id,
                config_name=l,
                private=True
            )

def hybrid_split(string: str):
    """
    Split a string into tokens using a hybrid regex.

    Args:
        string (str): Input string.

    Returns:
        List[str]: List of tokens.
    """
    regex = r"[\u4e00-\ufaff]|[0-9]+|[a-zA-Z]+\'*[a-z]*"
    matches = re.findall(regex, string, re.UNICODE)
    return matches

def load_asr_pipeline(repo) -> transformers.Pipeline:
    """
    Load the ASR pipeline from Hugging Face Hub.

    Returns:
        Pipeline: Loaded ASR pipeline.
    """
    if os.path.exists(f"./temp/{repo}"):
        asr_pipe = transformers.pipeline("automatic-speech-recognition", model=f"./temp/{repo}")
    else:
        print("Downloading ASR Model...")
        asr_pipe = transformers.pipeline("automatic-speech-recognition", model=repo)
        asr_pipe.save_pretrained(f"./temp/{repo}")
    return asr_pipe

def load_med_list_pipeline(repo) -> transformers.Pipeline:
    """
    Load the medicine list pipeline from Hugging Face Hub.

    Returns:
        Pipeline: Loaded medicine list pipeline.
    """
    if os.path.exists(f"./temp/{repo}"):
        med_list_pipe = transformers.pipeline("token-classification", model=f"./temp/{repo}")
    else:
        print("Downloading NER Model...")
        med_list_pipe = transformers.pipeline("token-classification", model=repo)
        med_list_pipe.save_pretrained(f"./temp/{repo}")
    return med_list_pipe

def load_intent_pipeline(repo="MedicGrabber_multitask_BERT_intent") -> transformers.Pipeline:
    """
    Load the intent pipeline from Hugging Face Hub.

    Returns:
        Pipeline: Loaded intent pipeline.
    """
    if os.path.exists(f"./temp/{repo}"):
        intent_pipe = transformers.pipeline("text-classification", model=f"./temp/{repo}")
    else:
        print("Downloading Intent Model...")
        intent_pipe = transformers.pipeline("text-classification", model=repo)
        intent_pipe.save_pretrained(f"./temp/{repo}")
    return intent_pipe

def listen_audio_thread(model_dict: dict, shared_dict: dict, listen_event) -> None:
    
    asr_pipe = model_dict["asr_pipe"]
    intent_pipe = model_dict["intent_pipe"]
    med_pipe = model_dict["med_list_pipe"]

    while True:

        listen_event.wait()

        # # Idle when grabbing medicine
        # if (shared_dict["user_flag"] and shared_dict["current_cmd"]) or shared_dict["play_sound_flag"]:
        #     # print("IP Thread: Idle")
        #     time.sleep(shared_dict["THREAD_PROCESS_TIMER"])
        #     continue

        audio_array = None
        transcript = None

        # print("\nRecording for 5 seconds...")
        audio_data = sd.rec(int(shared_dict["THREAD_PROCESS_TIMER"] * 16000), samplerate=16000, channels=1, dtype="float32")
        sd.wait()  # Wait until recording is finished
        audio_array = np.squeeze(audio_data)  # Convert to 1D array

        transcript = asr_pipe(audio_array)["text"]
        # print("Transcript:", transcript)

        if len(transcript) > 0 :

            intent = intent_pipe(transcript)[0]["label"][-1]
            med_list = New_PharmaIntent_Dataset.post_process_med(med_pipe(transcript))
            # print(intent, med_list)

            clean_meds = []
            for med in med_list:
                if med != "Empty":
                    clean_meds.append(med)

            # The second printing line should be audio
            if  intent == "1" and len(clean_meds) > 0:
                # print("Command Heard: Retrieve Medicine")
                print("\nRetrieving medicine. Please wait while I get it for you.")
                shared_dict["current_cmd"] = True
                shared_dict["queued_objects"] = shared_dict["queued_objects"] + clean_meds
            
            elif intent == "2" and len(clean_meds) > 0:
                # print("Command Heard: Search Medicine")
                print("\nSeems like you are looking for a medicine. Please wait while I search for it.")
                # shared_dict["current_cmd"] = True
                # shared_dict["queued_objects"] = shared_dict["queued_objects"] + clean_meds

            elif intent == "3":
                # print("Command Heard: Enquire Suitable Medicine")
                print("\nMy apologies, I am not able to diagnosis medical issues. Please consult to professional to get the best advice.")
            
            elif intent in ["1", "2"] and len(clean_meds) == 0:
                # print("Command Heard: Retrieve/Search Medicine")
                print("\nSorry, I only retrieve designated medicines. Please try again.")

            elif intent in ["0", "3"] and len(clean_meds) > 0:
                # print("Command Heard: Other Intents")
                print("\nHeard that you mentioned about chronic disease medicines. If you search for them, please let me know.")

        else:
            print("No speech detected.")
        
        # Wait 1 second before looping again 
        time.sleep(2)

def live_test(model_dict):

    asr_pipe = model_dict["asr_pipe"]
    intent_pipe = model_dict["intent_pipe"]
    med_pipe = model_dict["med_list_pipe"]

    while True:

        audio_array = None
        transcript = None

        # print("\nRecording for 5 seconds...")
        audio_data = sd.rec(int(5 * 16000), samplerate=16000, channels=1, dtype="float32")
        sd.wait()  # Wait until recording is finished
        audio_array = np.squeeze(audio_data)  # Convert to 1D array

        transcript = asr_pipe(audio_array)["text"]
        # print("Transcript:", transcript)
        intent = -1
        clean_meds = []

        valid_transcript = len(hybrid_split(transcript)) > 1

        if valid_transcript:

            intent = intent_pipe(transcript)[0]["label"][-1]
            med_list = New_PharmaIntent_Dataset.post_process_med(med_pipe(transcript))
            # print(intent, med_list)
            for med in med_list:
                if med != "Empty":
                    clean_meds.append(med)
        
        intent_label = New_PharmaIntent_Dataset.INTENT_LABEL[int(intent)] if valid_transcript else "None"

        print(f"Detected Medicines: {clean_meds} | Intent: {intent_label}")

def main():

    model_dict = {
        "asr_pipe":             load_asr_pipeline("borisPMC/MedicGrabber_WhisperSmall"),
        "med_list_pipe":        load_med_list_pipeline("borisPMC/MedicGrabber_multitask_BERT_ner"),
        "intent_pipe":          load_intent_pipeline("borisPMC/MedicGrabber_multitask_BERT_intent"),
    }

    model_dict["asr_pipe"].generation_config.forced_decoder_ids = None

    live_test(model_dict)

if __name__ == "__main__":
    main()
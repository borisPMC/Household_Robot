{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Attempt to mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"Error mounting Google Drive:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aT13EOJXKrU",
        "outputId": "862f05f1-caa9-48b2-b82c-c934912df296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n",
        "\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    print(\"YOLO already installed!\")\n",
        "except ImportError:\n",
        "    print(\"Installing YOLO...\")\n",
        "    !pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31hiG6RdQLVH",
        "outputId": "4c1afe95-969f-4f5a-de76-42e03d0058db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO already installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: ViTPose"
      ],
      "metadata": {
        "id": "sQcKxhkkY-QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseEstimator_ViTPose:\n",
        "    def __init__(self, device=None):\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
        "        self.person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=self.device)\n",
        "        self.pose_image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n",
        "        self.pose_model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=self.device)\n",
        "\n",
        "    # For the load_image method, it does not have to input an url, as long as it returns an image object\n",
        "    def load_image(self, fpath):\n",
        "        return Image.open(fpath)\n",
        "\n",
        "    def detect_humans(self, image):\n",
        "        inputs = self.person_image_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.person_model(**inputs)\n",
        "        results = self.person_image_processor.post_process_object_detection(\n",
        "            outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n",
        "        )\n",
        "        result = results[0]  # take first image results\n",
        "        person_boxes = result[\"boxes\"][result[\"labels\"] == 0].cpu().numpy()\n",
        "        person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n",
        "        person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n",
        "        return person_boxes\n",
        "\n",
        "    def detect_keypoints(self, image, person_boxes):\n",
        "        inputs = self.pose_image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.pose_model(**inputs)\n",
        "        pose_results = self.pose_image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\n",
        "        return pose_results[0]  # results for first image\n",
        "\n",
        "    def estimate_pose(self, fpath):\n",
        "        image = self.load_image(fpath)\n",
        "        person_boxes = self.detect_humans(image)\n",
        "        keypoints = self.detect_keypoints(image, person_boxes)\n",
        "        return keypoints"
      ],
      "metadata": {
        "id": "eMEWG8nQVbVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: YOLO"
      ],
      "metadata": {
        "id": "NuiQV6CBZEzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseEstimator_YOLOv8:\n",
        "    def __init__(self, model_path, device=\"cuda:0\"):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, data_path, epochs=100, img_size=640):\n",
        "        train_results = self.model.train(\n",
        "            data=data_path,\n",
        "            epochs=epochs,\n",
        "            imgsz=img_size,\n",
        "            device=self.device\n",
        "        )\n",
        "        return train_results\n",
        "\n",
        "    def evaluate(self):\n",
        "        metrics = self.model.val()\n",
        "        return metrics\n",
        "\n",
        "    def detect(self, image_path):\n",
        "        results = self.model(image_path)\n",
        "        return results\n",
        "\n",
        "    def show_results(self, results):\n",
        "        results[0].show()\n",
        "\n",
        "    def export_model(self, format=\"onnx\"):\n",
        "        path = self.model.export(format=format)\n",
        "        return path"
      ],
      "metadata": {
        "id": "sy-U8S_kVcCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_detected(keypoints, confidence_threshold=0.5):\n",
        "    # Check if the list is empty\n",
        "    if not keypoints:\n",
        "        return False\n",
        "\n",
        "    # Extract the first dictionary in the list\n",
        "    keypoint_data = keypoints[0]\n",
        "\n",
        "    # Check if 'keypoints' tensor is empty\n",
        "    if keypoint_data['keypoints'].numel() == 0:\n",
        "        return False\n",
        "\n",
        "    # Check if any keypoint has a confidence score above the threshold\n",
        "    scores = keypoint_data['scores']\n",
        "    if scores.numel() > 0 and (scores > 0.5).any():\n",
        "        return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "TogdiBCzVfo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, pose_estimator, confidence_threshold=0.5):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return False\n",
        "\n",
        "    # Get the video's FPS\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_interval = int(fps * 1)  # Process one frame every 1 second\n",
        "    print(f\"Processing every {frame_interval} frames (FPS: {fps})\")\n",
        "\n",
        "    frame_count = 0  # Counter for processed frames\n",
        "    total_detections = 0  # Counter for total detections\n",
        "\n",
        "    while True:\n",
        "        # Read a frame from the video\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # Exit the loop if no more frames are available\n",
        "\n",
        "        # Process only frames at 0.5-second intervals\n",
        "        if frame_count % frame_interval != 0:\n",
        "            frame_count += 1\n",
        "            continue\n",
        "\n",
        "        # Save the frame temporarily for processing\n",
        "        temp_image_path = \"/content/temp_frame.jpg\"\n",
        "        cv2.imwrite(temp_image_path, frame)\n",
        "\n",
        "        # Perform pose estimation on the frame\n",
        "        keypoints = pose_estimator.estimate_pose(temp_image_path)\n",
        "\n",
        "        # Check if a person is detected in this frame\n",
        "        if is_detected(keypoints, confidence_threshold):\n",
        "            print(f\"Person detected at {frame_count / fps:.2f} seconds\")\n",
        "            total_detections += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Total detections: {total_detections}\")\n",
        "    return total_detections > 0  # Return True if at least one detection occurred"
      ],
      "metadata": {
        "id": "sHGJtoqbp4jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def main():\n",
        "\n",
        "    '''\n",
        "    # Test image path\n",
        "    image_path = \"/content/drive/MyDrive/FYP/test_image.jpg\"\n",
        "\n",
        "    # For YOLO model\n",
        "    yolo_model = PoseEstimator_YOLOv8(\"yolov8n-pose.pt\")\n",
        "    results = yolo_model.detect(image_path)\n",
        "    yolo_model.show_results(results)\n",
        "    print(\"YOLO Keypoints:\", results)\n",
        "\n",
        "    # For ViTPose model\n",
        "    pose_estimator = PoseEstimator_ViTPose()\n",
        "    keypoints = pose_estimator.estimate_pose(image_path)\n",
        "    print(\"ViTPose Keypoints:\", keypoints)\n",
        "\n",
        "    # To determine if there is a person in front of the camera\n",
        "    print(\"Person Detected:\", is_detected(keypoints))\n",
        "    '''\n",
        "\n",
        "    # Test video path\n",
        "    video_path = \"/content/drive/MyDrive/FYP/test_video.mp4\"\n",
        "\n",
        "    # Initialize the pose estimator (e.g., ViTPose or YOLO)\n",
        "    pose_estimator = PoseEstimator_ViTPose()  # Replace with your actual pose estimator\n",
        "\n",
        "    # Process the video\n",
        "    person_detected = process_video(video_path, pose_estimator, confidence_threshold=0.5)\n",
        "\n",
        "    # Output the final result\n",
        "    print(\"Person detected in video:\", person_detected)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5BHpLgmppTm",
        "outputId": "ce5fecfa-a24d-456d-9062-f3dea4c945f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing every 30 frames (FPS: 30.001244864932154)\n",
            "Person detected at 0.00 seconds\n",
            "Person detected at 1.00 seconds\n",
            "Person detected at 2.00 seconds\n",
            "Person detected at 3.00 seconds\n",
            "Person detected at 4.00 seconds\n",
            "Person detected at 5.00 seconds\n",
            "Person detected at 6.00 seconds\n",
            "Person detected at 7.00 seconds\n",
            "Person detected at 8.00 seconds\n",
            "Total detections: 9\n",
            "Person detected in video: True\n"
          ]
        }
      ]
    }
  ]
}
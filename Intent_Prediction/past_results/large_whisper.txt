TRAIN_BATCH_SIZE = 16
VAL_BATCH_SIZE = 4   # 8:2 Training-validation ratio -> 16 train 4 valid
MAX_STEPS = 300
EVAL_STEPS = 30

LLM_EPOCH = 3
ASR_EPOCH = 3

SEED = 42

Result:

Language: ["English", "Cantonese", "Eng_Can" ,"Can_Eng"]
eval_loss_1:[3.3236477375030518, 6.644419193267822, 0.28772324323654175, 0.5896164774894714]
eval_loss_2:[0.24330087006092072, 0.24254441261291504, 0.6419094800949097, 0.32511553168296814]
eval_loss_3:[0.11235006898641586, 0.16228768229484558, 0.2063366025686264, 0.25191977620124817]
eval_CER_1: [6.663316582914573, 284.7121212121212, 0.8297872340425532, 1.0]
eval_CER_2: [0.8655778894472361, 1.0, 1.0, 5.389380530973451]
eval_CER_3: [0.7286432160804021, 0.9242424242424242, 0.9042553191489362, 0.8672566371681416]

Final w/ BERT: 0.15

{'loss': 0.3927, 'grad_norm': inf, 'learning_rate': 4.166666666666667e-05, 'epoch': 1.0}
{'eval_loss': 0.5896164774894714, 'eval_CER': 1.0, 'eval_runtime': 6.8371, 'eval_samples_per_second': 2.048, 'eval_steps_per_second': 0.585, 'epoch': 1.0}
{'loss': 0.4082, 'grad_norm': 3.6065514087677, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'eval_loss': 0.32511553168296814, 'eval_CER': 5.389380530973451, 'eval_runtime': 8.304, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 0.482, 'epoch': 2.0}
{'loss': 0.2486, 'grad_norm': 2.151235342025757, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.0}
{'eval_loss': 0.25191977620124817, 'eval_CER': 0.8672566371681416, 'eval_runtime': 7.53, 'eval_samples_per_second': 1.859, 'eval_steps_per_second': 0.531, 'epoch': 3.0}
{'train_runtime': 471.4701, 'train_samples_per_second': 0.223, 'train_steps_per_second': 0.013, 'train_loss': 0.3498380333185196, 'epoch': 3.0}

Log:

Using device: cuda
Dataset loaded successfully! 



self.trainer = Seq2SeqTrainer(
  0%|                                                                                                                                                                                         | 0/21 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...
{'loss': 3.6702, 'grad_norm': 23.72568130493164, 'learning_rate': 4.523809523809524e-05, 'epoch': 1.0}
 33%|███████████████████████████████████████████████████████████                                                                                                                      | 7/21 [01:33<05:47, 24.84s/itD 
ue to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
{'eval_loss': 3.3236477375030518, 'eval_CER': 6.663316582914573, 'eval_runtime': 25.7473, 'eval_samples_per_second': 1.049, 'eval_steps_per_second': 0.272, 'epoch': 1.0}
{'loss': 1.2718, 'grad_norm': 2.9050557613372803, 'learning_rate': 3.095238095238095e-05, 'epoch': 2.0}
{'eval_loss': 0.24330087006092072, 'eval_CER': 0.8655778894472361, 'eval_runtime': 18.2901, 'eval_samples_per_second': 1.476, 'eval_steps_per_second': 0.383, 'epoch': 2.0}
{'loss': 0.1597, 'grad_norm': 1.084097146987915, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.0}
{'eval_loss': 0.11235006898641586, 'eval_CER': 0.7286432160804021, 'eval_runtime': 18.8621, 'eval_samples_per_second': 1.431, 'eval_steps_per_second': 0.371, 'epoch': 3.0}
{'train_runtime': 1186.0829, 'train_samples_per_second': 0.298, 'train_steps_per_second': 0.018, 'train_loss': 1.7005694650468373, 'epoch': 3.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [19:46<00:00, 56.48s/it]
114 30 40
Importing model: borisPMC/whisper_largeTurbo_grab_medicine_intent
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:03<00:00, 33.57 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 31.51 examples/s]
c:\Users\20051248d\Documents\GitHub\Human-Intention-Prediction\Models.py:146: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  self.trainer = Seq2SeqTrainer(
{'loss': 5.7748, 'grad_norm': inf, 'learning_rate': 4.761904761904762e-05, 'epoch': 1.0}
{'eval_loss': 6.644419193267822, 'eval_CER': 284.7121212121212, 'eval_runtime': 31.7851, 'eval_samples_per_second': 0.944, 'eval_steps_per_second': 0.252, 'epoch': 1.0}
{'loss': 1.3122, 'grad_norm': 3.403975009918213, 'learning_rate': 3.095238095238095e-05, 'epoch': 2.0}
{'eval_loss': 0.24254441261291504, 'eval_CER': 1.0, 'eval_runtime': 22.905, 'eval_samples_per_second': 1.31, 'eval_steps_per_second': 0.349, 'epoch': 2.0}
{'loss': 0.1923, 'grad_norm': 1.6569204330444336, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.0}
{'eval_loss': 0.16228768229484558, 'eval_CER': 0.9242424242424242, 'eval_runtime': 23.288, 'eval_samples_per_second': 1.288, 'eval_steps_per_second': 0.344, 'epoch': 3.0}
{'train_runtime': 1990.6575, 'train_samples_per_second': 0.172, 'train_steps_per_second': 0.011, 'train_loss': 2.426454703013102, 'epoch': 3.0}
████| 6/6 [00:00<00:00, 39.47 examples/s]                                             ███████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [33:10<00:00, 94.79s/it]
c:\Users\20051248d\Documents\GitHub\Human-Intention-Prediction\Models.py:146: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  self.trainer = Seq2SeqTrainer(
{'loss': 0.3093, 'grad_norm': 5.130495071411133, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}
{'eval_loss': 0.28772324323654175, 'eval_CER': 0.8297872340425532, 'eval_runtime': 1278.0599, 'eval_samples_per_second': 0.005, 'eval_steps_per_second': 0.002, 'epoch': 1.0}
 33%|█████████████████████████████████████████████████████████▎                                                                                                              50%|██████████████████████
       | 3/6 [2:15:41<2:40:48, 3216.05s/it]
{'loss': 0.3603, 'grad_norm': inf, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'eval_loss': 0.6419094800949097, 'eval_CER': 1.0, 'eval_runtime': 267.0871, 'eval_samples_per_second': 0.022, 'eval_steps_per_second': 0.007, 'epoch': 2.0}
{'loss': 0.3519, 'grad_norm': 6.928723335266113, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.0}
{'eval_loss': 0.2063366025686264, 'eval_CER': 0.9042553191489362, 'eval_runtime': 68.3141, 'eval_samples_per_second': 0.088, 'eval_steps_per_second': 0.029, 'epoch': 3.0}
{'train_runtime': 10955.8644, 'train_samples_per_second': 0.011, 'train_steps_per_second': 0.001, 'train_loss': 0.3404829800128937, 'epoch': 3.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [3:02:35<00:00, 1825.98s/it]
No files have been modified since last commit. Skipping to prevent empty commit.
35 14 11
Importing model: borisPMC/whisper_largeTurbo_grab_medicine_intent
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:01<00:00, 31.73 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 34.65 examples/s]
c:\Users\20051248d\Documents\GitHub\Human-Intention-Prediction\Models.py:146: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  self.trainer = Seq2SeqTrainer(
{'loss': 0.3927, 'grad_norm': inf, 'learning_rate': 4.166666666666667e-05, 'epoch': 1.0}
{'eval_loss': 0.5896164774894714, 'eval_CER': 1.0, 'eval_runtime': 6.8371, 'eval_samples_per_second': 2.048, 'eval_steps_per_second': 0.585, 'epoch': 1.0}
{'loss': 0.4082, 'grad_norm': 3.6065514087677, 'learning_rate': 2.5e-05, 'epoch': 2.0}
{'eval_loss': 0.32511553168296814, 'eval_CER': 5.389380530973451, 'eval_runtime': 8.304, 'eval_samples_per_second': 1.686, 'eval_steps_per_second': 0.482, 'epoch': 2.0}
{'loss': 0.2486, 'grad_norm': 2.151235342025757, 'learning_rate': 8.333333333333334e-06, 'epoch': 3.0}
{'eval_loss': 0.25191977620124817, 'eval_CER': 0.8672566371681416, 'eval_runtime': 7.53, 'eval_samples_per_second': 1.859, 'eval_steps_per_second': 0.531, 'epoch': 3.0}
{'train_runtime': 471.4701, 'train_samples_per_second': 0.223, 'train_steps_per_second': 0.013, 'train_loss': 0.3498380333185196, 'epoch': 3.0}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [07:51<00:00, 78.58s/it]
No files have been modified since last commit. Skipping to prevent empty commit.
Dataset loaded successfully! 

Device set to use cuda:0
Device set to use cuda:0
  0%|                                                                                                                                                                                        | 0/160 [00:00<?, ?it/s]C:\Users\20051248d\Documents\GitHub\Human-Intention-Prediction\.venv\Lib\site-packages\transformers\models\whisper\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.
  warnings.warn(
  6%|██████████▉                                                                                                                                                                    | 10/160 [00:11<02:55,  1.17s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160/160 [03:06<00:00,  1.17s/it]
Predicted: ['Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 
'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty']
Label: ['Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Amitriptyline', 'Empty', 'Metformin', 'Amitriptyline', 'Amitriptyline', 'Empty', 'Empty', 'Amitriptyline', 'Metformin', 'Atorvastatin', 'Atorvastatin', 'Amitriptyline', 'Atorvastatin', 'Metformin', 'Empty', 'Amitriptyline', 'Amitriptyline', 'Metformin', 'ACE Inhibitor', 'ACE Inhibitor', 'Amitriptyline', 'Empty', 'Empty', 'ACE Inhibitor', 'Amitriptyline', 'Amitriptyline', 'ACE Inhibitor', 'Atorvastatin', 'ACE Inhibitor', 'ACE Inhibitor', 'Empty', 'ACE Inhibitor', 'Atorvastatin', 'Metformin', 'Metformin', 'Metformin', 'Amitriptyline', 'Amitriptyline', 'Metformin', 'ACE Inhibitor', 'Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Metformin', 'Amitriptyline', 'Empty', 'Amitriptyline', 'Amitriptyline', 'Amitriptyline', 'Atorvastatin', 'Atorvastatin', 'Atorvastatin', 'Metformin', 'Atorvastatin', 'Empty', 'Amitriptyline', 'Atorvastatin', 'Empty', 'Empty', 'Amitriptyline', 'Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Amitriptyline', 'Amitriptyline', 'Amitriptyline', 'Atorvastatin', 'Amitriptyline', 'Amitriptyline', 'Amitriptyline', 'Atorvastatin', 'Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Empty', 'ACE Inhibitor', 'Atorvastatin', 'Atorvastatin', 'Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Empty', 'Amitriptyline', 'Atorvastatin', 'Amitriptyline', 'Atorvastatin', 'ACE Inhibitor', 'ACE Inhibitor', 'Amitriptyline', 'ACE Inhibitor', 'Atorvastatin', 'Metformin', 'Amitriptyline', 'Empty', 'Atorvastatin', 'ACE Inhibitor', 'Metformin', 'Amitriptyline', 'Metformin', 'Metformin', 'Atorvastatin', 'Empty', 'Atorvastatin', 'Amitriptyline', 'Amitriptyline', 'Metformin', 'Empty', 'Atorvastatin', 'Amitriptyline', 'Atorvastatin', 'Metformin', 'Amitriptyline', 'Amitriptyline', 'Empty', 'Metformin', 'Amitriptyline', 'ACE Inhibitor', 'Empty', 'ACE Inhibitor', 'Amitriptyline', 'Atorvastatin', 'ACE Inhibitor', 'ACE Inhibitor', 'Amitriptyline', 'Amitriptyline', 'Metformin', 'Metformin', 'Amitriptyline', 'Amitriptyline', 'Metformin', 'Amitriptyline', 'ACE Inhibitor', 'Amitriptyline', 'Amitriptyline', 'Empty', 'ACE Inhibitor', 'Amitriptyline', 'Empty', 'Metformin', 'Atorvastatin', 'Metformin', 'Empty', 'Empty', 'ACE Inhibitor', 'Atorvastatin', 'Atorvastatin', 'Empty', 'Metformin', 'Amitriptyline', 'Metformin', 'Empty', 'ACE Inhibitor', 'Amitriptyline', 'Metformin', 'Metformin', 'Metformin', 'Amitriptyline']
0.15